{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/main/14-VAEs/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmgrepI5XCCQ"
   },
   "source": [
    "# Variational AutoEncoder (VAE)\n",
    "\n",
    "The VAE implemented in this tutorial is based on the following idea: a multivariate Normal distribution for the conditional distribution of the latent vectors given and input image ($q_{\\phi}(z | x_i)$ in the slides) and a multivariate Bernoulli distribution for the conditional distribution of images given the latent vector ($p_{\\theta}(x | z)$ in the slides). Using a Bernoulli distribution, the reconstruction loss (negative log likelihood of a data point in the output distribution) reduces to the pixel-wise binary cross-entropy. See the [original VAE paper](https://arxiv.org/pdf/1312.6114.pdf), Appendix C.1 for details. However, we are going to use MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load (or download) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://cseweb.ucsd.edu/~weijian/static/datasets/celeba/img_align_celeba.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip img_align_celeba.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"../13-GANs/data/\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((image_size, image_size)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob02Xn_0Xaj8"
   },
   "source": [
    "# Define our model\n",
    "\n",
    "\n",
    "In its original form, VAEs sample from a random node $z$ which is approximated by the parametric model $q(z∣ϕ,x)$ of the true posterior. Backprop cannot flow through a random node.\n",
    "\n",
    "Introducing a new parameter $\\epsilon$ allows us to reparameterize $z$ in a way that allows backprop to flow through the deterministic nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1400/1*ZlzFeen0J7Ize__drfbwOQ.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbnwinMlXbAb"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, nc, nef, nz, isize, device):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Encoder: (nc, isize, isize) -> (nef*8, isize//16, isize//16)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, nef, 4, 2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(nef),\n",
    "\n",
    "            nn.Conv2d(nef, nef*2, 4, 2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(nef*2),\n",
    "\n",
    "            nn.Conv2d(nef*2, nef*4, 4, 2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(nef*4),\n",
    "\n",
    "            nn.Conv2d(nef*4, nef*8, 4, 2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(nef*8)\n",
    "        )\n",
    "\n",
    "        # Map the encoded feature map to the latent vector of mean, (log)variance\n",
    "        out_size = isize // 16\n",
    "        self.mean = nn.Linear(nef*8*out_size*out_size, nz)\n",
    "        self.logvar = nn.Linear(nef*8*out_size*out_size, nz)\n",
    "\n",
    "    @staticmethod\n",
    "    def reparametrize(mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Batch size\n",
    "        batch_size = inputs.size(0)\n",
    "        # Encoded feature map\n",
    "        hidden = self.encoder(inputs)\n",
    "        # Reshape\n",
    "        hidden = hidden.view(batch_size, -1)\n",
    "        # Calculate mean and (log)variance\n",
    "        mean, logvar = self.mean(hidden), self.logvar(hidden)\n",
    "        # Sample\n",
    "        latent_z = self.reparametrize(mean, logvar)\n",
    "\n",
    "        return latent_z, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, nc, ndf, nz, isize):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Map the latent vector to the feature map space\n",
    "        self.ndf = ndf\n",
    "        self.out_size = isize // 16\n",
    "        self.decoder_dense = nn.Sequential(\n",
    "            nn.Linear(nz, ndf*8*self.out_size*self.out_size),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        # Decoder: (ndf*8, isize//16, isize//16) -> (nc, isize, isize)\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf*8, ndf*4, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(ndf*4, 1.e-3),\n",
    "\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf*4, ndf*2, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(ndf*2, 1.e-3),\n",
    "        \n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf*2, ndf, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.BatchNorm2d(ndf, 1.e-3),\n",
    "\n",
    "            nn.UpsamplingNearest2d(scale_factor=2),\n",
    "            nn.Conv2d(ndf, nc, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        hidden = self.decoder_dense(input).view(\n",
    "            batch_size, self.ndf*8, self.out_size, self.out_size)\n",
    "        output = self.decoder_conv(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=32, nef=32, nz=100, isize=64, device=torch.device(\"cuda:0\"), is_train=True):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.nz = nz\n",
    "        self.isize=isize\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(nc=nc, nef=nef, nz=nz, isize=isize, device=device)\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(nc=nc, ndf=ndf, nz=nz, isize=isize)\n",
    "\n",
    "        if is_train == False:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.decoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent_z, mean, logvar = self.encoder(x)\n",
    "        rec_x = self.decoder(latent_z)\n",
    "        return rec_x, mean, logvar\n",
    "    \n",
    "    def encode(self, x):\n",
    "        latent_z, _, _ = self.encoder(x)\n",
    "        return latent_z\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def reparametrize(mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def sample(self, size):\n",
    "        sample = torch.randn(size, self.nz).to(self.device)\n",
    "        return model.decode(sample)\n",
    "    \n",
    "    @property\n",
    "    def device(self): return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM99pbkVbQ27"
   },
   "source": [
    "## Define Loss function \n",
    "\n",
    "The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties: continuity (two close points in the latent space should not give two completely different contents once decoded) and completeness (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded).\n",
    "\n",
    "![](https://machinelearningmastery.ru/img/0-609638-740406.png)\n",
    "\n",
    "The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, to “ignore” the fact that distributions are returned and behave almost like classic autoencoders (leading to overfitting). We have to regularise both the covariance matrix and the mean of the distributions returned by the encoder. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others.\n",
    "\n",
    "![](https://machinelearningmastery.ru/img/0-957522-896652.png)\n",
    "\n",
    "The parameters of a VAE are trained via two loss functions: a reconstruction loss that forces the decoded samples to match the initial inputs, and a regularization loss that helps learn well-formed latent spaces and reduce overfitting to the training data.\n",
    "\n",
    "$$L = \\int_zq(z)log(p(x|\\Theta,z))dz - KL(q(z)||p(z))$$\n",
    "\n",
    "$$p(z)=\\mathcal{N}(0,1)$$\n",
    "$$q(z)=\\mathcal{N}(\\mu_1,\\Sigma_1) , \\Sigma_1 \\text{is diagonal matrix.}$$\n",
    "\n",
    "\n",
    "$${KL}[\\mathcal{N}(\\mu_0, \\Sigma_0)||\\mathcal{N}(\\mu_1, \\Sigma_1)] = \\frac{1}{2}\\big(tr(\\Sigma_1^{-1}\\Sigma_0)+(\\mu_1-\\mu_0)^T\\Sigma_1^{-1}(\\mu_1-\\mu_0)-k+\\log(\\frac{\\det\\Sigma_1}{\\det\\Sigma_0})\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    batch_size = recon_x.shape[0]\n",
    "    MSE = F.mse_loss(recon_x.view(batch_size,-1), x.view(batch_size, -1), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "8eed5a5b79c141518d5617010c90a256",
      "b367a29430d245f1be811f6a6f766364",
      "e460c14e8aa74f7fa5a087a44c87a38f",
      "48c6f6e0543e441fa3310256c54b8e0a",
      "a3f1ebca8cb94055bf7a43501fb47336",
      "43a27290c913474c86ca7275a39d73ec",
      "70a64def9e6040f5adf1839aa2cb7262",
      "3b6de232b2bb43c7abcbd3d1e53a86fc"
     ]
    },
    "id": "fnWLi1d0aS8y",
    "outputId": "5267c145-74ae-4fc0-b75d-2804c5f8574d"
   },
   "outputs": [],
   "source": [
    "epochs=1\n",
    "\n",
    "def train(pbar):\n",
    "    model.train()\n",
    "    train_mse, train_kld, train_loss = 0, 0, 0\n",
    "    for batch_idx, (data, _) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        mse_loss, kld_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = mse_loss + kld_loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.item();\n",
    "        train_mse += mse_loss.item(); train_kld += kld_loss.item()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"[KL: {kld_loss.item()/len(data) :.2f}] [MSE: {mse_loss.item()/len(data): .2f}]\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "with tqdm(total=epochs*len(dataloader), desc=f\"[KL: ?] [MSE: ?]\") as pbar:\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(pbar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.clamp(0, 1)\n",
    "    return x\n",
    "\n",
    "def show_image(img):\n",
    "    img = to_img(img)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def visualise_output(images, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        images = images.to(device)\n",
    "        images, _, _ = model(images)\n",
    "        images = images.cpu()\n",
    "        images = to_img(images)\n",
    "        np_imagegrid = torchvision.utils.make_grid(images[:50], 10, 5, normalize=True).numpy()\n",
    "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(dataloader).next()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Original images')\n",
    "show_image(torchvision.utils.make_grid(images[:64],padding=5, normalize=True))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"VAE reconstruction\")\n",
    "images = images.to(device)\n",
    "images, _, _ = model(images)\n",
    "images = images.cpu()\n",
    "show_image(torchvision.utils.make_grid(images[:64], padding=5, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Arithmetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(lambda1, model, img1, img2):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # latent vector of first image\n",
    "        img1 = img1.to(device).view(-1, 3, 64, 64)\n",
    "        latent_1 = model.encode(img1)\n",
    "\n",
    "        # latent vector of second image\n",
    "        img2 = img2.to(device).view(-1, 3, 64, 64)\n",
    "        latent_2 = model.encode(img2)\n",
    "\n",
    "        # interpolation of the two latent vectors\n",
    "        inter_latent = lambda1* latent_1 + (1- lambda1) * latent_2\n",
    "\n",
    "        # reconstruct interpolated image\n",
    "        inter_image = model.decode(inter_latent)\n",
    "        inter_image = inter_image.cpu()\n",
    "\n",
    "        return inter_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation lambdas\n",
    "lambda_range=np.linspace(0,1,10)\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "axs = axs.ravel()\n",
    "\n",
    "for ind,l in enumerate(lambda_range):\n",
    "    inter_image=interpolation(float(l), model, real_batch[0][0], real_batch[0][1])   \n",
    "    inter_image = to_img(inter_image)\n",
    "    \n",
    "    \n",
    "    axs[ind].imshow(np.transpose(inter_image[0], (1, 2, 0)))\n",
    "    axs[ind].set_title('lambda_val='+str(round(l,1)))\n",
    "    axs[ind].axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # sample latent vectors from the normal distribution\n",
    "    latent = torch.randn(128, 100, device=device)\n",
    "\n",
    "    # reconstruct images from the latent vectors\n",
    "    img_recon = model.decode(latent)\n",
    "    img_recon = img_recon.cpu().reshape(-1, 3, 64, 64)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    show_image(torchvision.utils.make_grid(img_recon.data[:100],10,5, normalize=True))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # create a sample grid in latent space\n",
    "    sample = np.random.normal(size = (100))\n",
    "    \n",
    "    latent_x = np.linspace(-2,2,10)\n",
    "    latent_y = np.linspace(-2,2,10)\n",
    "    \n",
    "    latents = torch.FloatTensor(len(latent_y), len(latent_x), 100)\n",
    "    \n",
    "    for i, lx in enumerate(latent_x):\n",
    "        for j, ly in enumerate(latent_y):\n",
    "            latents[j, i,] = torch.from_numpy(sample)\n",
    "            for z in range(n_features):            \n",
    "                latents[j, i, 2 * z] = lx\n",
    "                latents[j, i, 2 * z + 1] = ly\n",
    "\n",
    "    latents = latents.view(-1, 100) # flatten grid into a batch\n",
    "\n",
    "    # reconstruct images from the latent vectors\n",
    "    latents = latents.to(device)\n",
    "    image_recon = model.decode(latents)\n",
    "    image_recon = image_recon.cpu()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    show_image(torchvision.utils.make_grid(image_recon.data,10,5, normalize=True))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3b6de232b2bb43c7abcbd3d1e53a86fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43a27290c913474c86ca7275a39d73ec": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48c6f6e0543e441fa3310256c54b8e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b6de232b2bb43c7abcbd3d1e53a86fc",
      "placeholder": "​",
      "style": "IPY_MODEL_70a64def9e6040f5adf1839aa2cb7262",
      "value": " 8648/31650 [17:44&lt;47:12,  8.12it/s]"
     }
    },
    "70a64def9e6040f5adf1839aa2cb7262": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8eed5a5b79c141518d5617010c90a256": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e460c14e8aa74f7fa5a087a44c87a38f",
       "IPY_MODEL_48c6f6e0543e441fa3310256c54b8e0a"
      ],
      "layout": "IPY_MODEL_b367a29430d245f1be811f6a6f766364"
     }
    },
    "a3f1ebca8cb94055bf7a43501fb47336": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b367a29430d245f1be811f6a6f766364": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e460c14e8aa74f7fa5a087a44c87a38f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "[KL: 136.10] [MSE:  381.62]:  27%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43a27290c913474c86ca7275a39d73ec",
      "max": 31650,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a3f1ebca8cb94055bf7a43501fb47336",
      "value": 8648
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
